# Pipeline de ETL com PySpark para Dados JSON

Este reposit√≥rio cont√©m um pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) robusto, constru√≠do em Python e PySpark, projetado para processar dados semiestruturados em formato JSON. O projeto serve como um exemplo pr√°tico de como ingerir dados de uma API p√∫blica, normalizar estruturas complexas e aninhadas, e salvar o resultado em um formato otimizado para an√°lise de dados.

## üéØ Objetivo do Projeto

O objetivo principal √© demonstrar a constru√ß√£o de um pipeline de dados resiliente e eficiente, capaz de lidar com desafios comuns em engenharia de dados, como:

  * **Ingest√£o de dados via API:** Conectar-se a um endpoint web para extrair dados.
  * **Processamento de JSON aninhado:** Lidar com objetos, arrays e mapas dentro de uma mesma estrutura de dados.
  * **Normaliza√ß√£o e Limpeza:** Transformar dados brutos e complexos em um formato tabular (colunar) limpo e pronto para uso.
  * **Toler√¢ncia a falhas:** Garantir que o pipeline lide de forma graciosa com dados ausentes ou malformados.
  * **Boas pr√°ticas de engenharia de software:** Utilizar logging, modulariza√ß√£o e configura√ß√£o para criar um c√≥digo leg√≠vel e de f√°cil manuten√ß√£o.

Para este projeto, utilizamos a API p√∫blica **[REST Countries](https://restcountries.com/)**, que fornece informa√ß√µes detalhadas sobre os pa√≠ses do mundo em um formato JSON rico e complexo.

## ‚ú® Principais Funcionalidades

  * **Extra√ß√£o de API:** Conecta-se √† API REST Countries e baixa os dados de todos os pa√≠ses.
  * **Tratamento de Erros de Rede:** O pipeline √© capaz de lidar com erros HTTP e de conex√£o durante a extra√ß√£o.
  * **Valida√ß√£o de Schema:** Antes do processamento, o script valida se os campos essenciais existem nos dados de origem, garantindo que o pipeline falhe rapidamente em caso de mudan√ßas na estrutura da API.
  * **Transforma√ß√£o Robusta com `explode_outer`:** Utiliza `explode_outer` para achatar mapas e arrays, garantindo que nenhum registro seja perdido caso um pa√≠s n√£o tenha informa√ß√µes de moeda ou idioma.
  * **Limpeza e Padroniza√ß√£o com `coalesce`:** Campos nulos (`null`) resultantes da transforma√ß√£o s√£o substitu√≠dos por valores padr√£o (ex: "N/A" ou 0), garantindo a consist√™ncia dos dados.
  * **Tipagem de Dados:** Realiza a convers√£o expl√≠cita de colunas para os tipos de dados corretos (Integer, Double), otimizando o armazenamento e a performance de consultas.
  * **Logging Estruturado:** Registra todas as etapas importantes, sucessos e erros da execu√ß√£o, facilitando o monitoramento e a depura√ß√£o.
  * **Sa√≠da em Formato Otimizado:** Salva o DataFrame final no formato **Parquet**, que √© altamente eficiente para workloads anal√≠ticos em ecossistemas de Big Data.

## ‚öôÔ∏è Tecnologias Utilizadas

  * **Python:** Linguagem principal para orquestra√ß√£o do pipeline.
  * **Apache Spark (PySpark):** Ferramenta de processamento de dados distribu√≠do usada para a transforma√ß√£o em larga escala.
  * **Requests:** Biblioteca Python para realizar as chamadas √† API HTTP.
  * **Findspark:** Utilit√°rio para facilitar a localiza√ß√£o da instala√ß√£o do Spark em ambientes locais.

## üöÄ Como Executar o Pipeline

Siga os passos abaixo para executar o projeto em seu ambiente local.

### 1\. Pr√©-requisitos

  * **Python 3.9 ou superior.**
  * **Java Development Kit (JDK) 8 ou 11**, que √© uma depend√™ncia do Apache Spark.
  * Clone este reposit√≥rio:
    ```bash
    git clone https://github.com/seu-usuario/nome-do-repositorio.git
    cd nome-do-repositorio
    ```
  * Instale as bibliotecas Python necess√°rias:
    ```bash
    pip install pyspark findspark requests
    ```

### 2\. Execu√ß√£o

Com todos os pr√©-requisitos instalados, execute o script principal do pipeline:

```bash
python pipeline.py # ou o nome que voc√™ deu ao arquivo .py
```

### 3\. Verificar a Sa√≠da

Ap√≥s a execu√ß√£o bem-sucedida, os seguintes artefatos ser√£o criados no diret√≥rio do projeto:

  * üìÑ **`dados_brutos_paises.json`**: Um arquivo JSON contendo os dados brutos e n√£o modificados, exatamente como vieram da API.
  * üìÅ **`dados_processados_paises.parquet/`**: Um diret√≥rio contendo os dados processados e limpos no formato Parquet. Este diret√≥rio pode ser lido por qualquer ferramenta de an√°lise compat√≠vel com Parquet (Spark, Pandas, DuckDB, etc.).

## üì¶ Arquitetura do Pipeline

O pipeline segue as tr√™s etapas cl√°ssicas do processo de ETL:

### 1\. Extra√ß√£o (Extract)

  * A fun√ß√£o `extrair_dados_api` √© chamada.
  * Ela envia uma requisi√ß√£o `GET` para a URL da API `restcountries.com/v3.1/all`.
  * O JSON de resposta √© salvo localmente no arquivo `dados_brutos_paises.json`.
  * Tratamentos de erro verificam se a requisi√ß√£o foi bem-sucedida.

### 2\. Transforma√ß√£o (Transform)

  * Uma `SparkSession` √© iniciada.
  * O arquivo `dados_brutos_paises.json` √© carregado em um DataFrame Spark. O Spark infere o schema aninhado automaticamente.
  * **Valida√ß√£o de Schema:** A estrutura do DataFrame √© verificada para garantir que os campos-chave (`name`, `currencies`, etc.) est√£o presentes.
  * **Achatamento (Flattening):**
      * `explode_outer(col("currencies"))`: Transforma o mapa de moedas. Para cada pa√≠s, cria uma nova linha para cada moeda. Se um pa√≠s n√£o tiver moeda, a linha √© mantida com valores nulos.
      * `explode_outer(col("languages"))`: O mesmo processo √© aplicado para os idiomas.
      * `col("name.common")`: Campos dentro de objetos aninhados s√£o acessados diretamente usando a nota√ß√£o de ponto.
      * `element_at(col("capital"), 1)`: O primeiro elemento do array de capitais √© extra√≠do.
  * **Limpeza e Convers√£o:**
      * `coalesce(col(...), lit(...))`: Garante que qualquer valor nulo seja substitu√≠do por um valor padr√£o.
      * `.cast(...)`: As colunas `population` e `area` s√£o convertidas para tipos num√©ricos.
  * O resultado √© um DataFrame limpo, tabular e com um schema bem definido.

### 3\. Carregamento (Load)

  * A fun√ß√£o `carregar_dados_parquet` √© executada.
  * O DataFrame transformado √© salvo no diret√≥rio `dados_processados_paises.parquet/` usando o modo `overwrite`, que substitui os dados de execu√ß√µes anteriores.

## üìà Melhorias Futuras

Este projeto √© uma base s√≥lida que pode ser estendida com funcionalidades mais avan√ßadas, como:

  * **Orquestra√ß√£o:** Integrar o pipeline com uma ferramenta como **Apache Airflow** ou **Mage** para agendar e monitorar execu√ß√µes de forma autom√°tica.
  * **Containeriza√ß√£o:** Empacotar a aplica√ß√£o e suas depend√™ncias em um container **Docker** para garantir portabilidade e consist√™ncia entre ambientes.
  * **Testes:** Implementar testes unit√°rios e de integra√ß√£o com `pytest` para validar a l√≥gica de transforma√ß√£o e garantir a qualidade dos dados.
  * **Gerenciamento de Configura√ß√£o:** Mover as configura√ß√µes (URLs, caminhos, etc.) para arquivos externos, como `.yaml` ou `.env`.
  * **Schema Registry:** Utilizar um Schema Registry para gerenciar e validar a evolu√ß√£o do schema dos dados ao longo do tempo.

-----
